"""
Scraper Winamax - R√©cup√®re les cotes depuis winamax.fr avec Selenium + BeautifulSoup
Ce module est sp√©cifique √† Winamax.
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import re
from typing import List, Optional
import time
import sys
import os

# Ajouter le parent au path pour importer models
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models import Match, ScraperResult, display_matches


class WinamaxScraper:
    """
    Scraper pour winamax.fr avec Selenium + BeautifulSoup
    
    Utilise Chrome headless pour charger la page puis BeautifulSoup
    pour parser le HTML et extraire les cotes 1X2.
    """
    
    BOOKMAKER_NAME = "Winamax"
    BASE_URL = "https://www.winamax.fr"
    
    # Sports 1X2 (3 joueurs) - Football, Rugby, Hockey
    SPORTS_1X2 = {
        "Football": "/paris-sportifs/sports/1",
        "Rugby": "/paris-sportifs/sports/12",
        "Hockey": "/paris-sportifs/sports/4",
    }
    
    # Sports 1-2 (2 joueurs) - Basketball, Tennis
    SPORTS_1_2 = {
        "Basketball": "/paris-sportifs/sports/2",
        "Tennis": "/paris-sportifs/sports/5",
    }
    
    # Ancien nom pour compatibilit√©
    FOOTBALL_PAGES = {
        "Football": "/paris-sportifs/sports/1",
    }
    
    def __init__(self, headless: bool = True, fast_mode: bool = True):
        """Initialise le scraper Winamax
        
        Args:
            headless: True pour ex√©cuter sans interface graphique
            fast_mode: True pour scraper seulement les pages principales
        """
        self.headless = headless
        self.fast_mode = fast_mode
        self.driver = None
        self.cookies_accepted = False
    
    def _create_driver(self):
        """Cr√©e un driver Chrome avec options anti-d√©tection"""
        options = Options()
        if self.headless:
            options.add_argument("--headless=new")
        
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        service = Service(ChromeDriverManager().install())
        # Initialisation du driver
        driver = webdriver.Chrome(service=service, options=options)
        
        # Configuration Stealth (Furtivit√© avanc√©e)
        try:
            from selenium_stealth import stealth
            stealth(driver,
                languages=["fr-FR", "fr"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True,
            )
        except ImportError:
            print("‚ö†Ô∏è Selenium-stealth non install√©, mode standard")

        # Bypass classique suppl√©mentaire
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """
        })
        
        return driver
    
    def _start_driver(self):
        """D√©marre le driver si n√©cessaire"""
        if self.driver is None:
            self.driver = self._create_driver()
    
    def _stop_driver(self):
        """Arr√™te le driver"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None
    
    def _accept_cookies(self):
        """Accepte les cookies si la popup appara√Æt"""
        if self.cookies_accepted:
            return
        
        try:
            # Winamax utilise diff√©rents s√©lecteurs pour les cookies
            cookie_selectors = [
                "//button[contains(text(), 'Tout accepter')]",
                "//button[contains(text(), 'Accepter')]",
                "//button[contains(@class, 'accept')]",
                "//button[@id='tarteaucitronPersonalize2']",
            ]
            
            for selector in cookie_selectors:
                try:
                    cookie_btn = WebDriverWait(self.driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    cookie_btn.click()
                    self.cookies_accepted = True
                    time.sleep(1)
                    break
                except:
                    continue
        except:
            pass
    
    def scrape(self) -> ScraperResult:
        """Lance le scraping et retourne un ScraperResult"""
        start_time = time.time()
        all_matches = []
        status = "success"
        message = ""
        
        print(f"üîÑ Scraping {self.BOOKMAKER_NAME} (mode {'rapide' if self.fast_mode else 'complet'})...")
        
        try:
            self._start_driver()
            
            # Scraper tous les sports 1X2 (foot, rugby, hockey) et 1-2 (basket, tennis)
            sports_to_scrape = {**self.SPORTS_1X2, **self.SPORTS_1_2}
            for sport_name, sport_path in sports_to_scrape.items():
                matches = self._scrape_page(sport_name, sport_path)
                
                new_count = 0
                for match in matches:
                    # Ajouter le sport au match
                    match.sport = sport_name
                    # √âviter les doublons par ID unique (d√©j√† normalis√©)
                    if not any(m.id == match.id for m in all_matches):
                        all_matches.append(match)
                        new_count += 1
                
                if new_count > 0:
                    print(f"  ‚úÖ {sport_name}: +{new_count} nouveaux matchs")
            
            message = f"{len(all_matches)} matchs r√©cup√©r√©s"
            
        except Exception as e:
            status = "error"
            message = str(e)
            print(f"‚ùå Erreur: {e}")
        finally:
            self._stop_driver()
        
        duration = time.time() - start_time
        print(f"\nüìä Total: {len(all_matches)} matchs uniques ({duration:.1f}s)")
        
        return ScraperResult(
            matches=all_matches,
            bookmaker=self.BOOKMAKER_NAME,
            status=status,
            message=message,
            duration_seconds=duration
        )
    
    def get_all_matches(self) -> List[Match]:
        """Alias pour compatibilit√©"""
        return self.scrape().matches
    
    def _scrape_page(self, name: str, path: str) -> List[Match]:
        """Scrape une page Winamax avec Selenium puis parse avec BeautifulSoup"""
        matches = []
        
        try:
            url = f"{self.BASE_URL}{path}"
            self.driver.get(url)
            time.sleep(2)  # R√©duit de 3 √† 2
            self._accept_cookies()
            time.sleep(1)  # R√©duit de 2 √† 1
            
            # Scroll pour charger plus de matchs
            self._scroll_page()
            
            # R√©cup√©rer le HTML et parser avec BeautifulSoup
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'lxml')
            
            matches = self._parse_matches_with_bs4(soup, name)
            print(f"    ‚Üí {len(matches)} matchs trouv√©s")
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Erreur: {str(e)[:50]}")
        
        return matches
    
    def _scroll_page(self):
        """Scroll la page pour charger plus de contenu"""
        try:
            for _ in range(3):
                self.driver.execute_script("window.scrollBy(0, 1000);")
                time.sleep(0.3)  # R√©duit de 0.5 √† 0.3
        except:
            pass
    
    def _parse_matches_with_bs4(self, soup: BeautifulSoup, competition: str) -> List[Match]:
        """Parse les matchs avec BeautifulSoup - adapt√© √† la structure Winamax"""
        matches = []
        
        # Winamax utilise la classe bet-group-outcome-odd pour les boutons de cotes
        bet_buttons = soup.select('.bet-group-outcome-odd')
        
        if not bet_buttons:
            # Fallback: chercher par classe contenant "odd"
            bet_buttons = soup.select('[class*="odd-button"]')
        
        print(f"    (trouv√© {len(bet_buttons)} boutons de cotes)")
        
        # Les cotes sont group√©es par 3 (1, N, 2)
        # Remonter de 2 niveaux pour trouver le conteneur des 3 cotes
        processed_grandparents = set()
        
        for bet_btn in bet_buttons:
            # Remonter de 2 niveaux (parent puis grand-parent)
            parent = bet_btn.parent
            if parent:
                grandparent = parent.parent
                if grandparent and id(grandparent) not in processed_grandparents:
                    # V√©rifier si le grand-parent contient exactement 2 ou 3 √©l√©ments de cotes
                    odds_in_grandparent = grandparent.select('.bet-group-outcome-odd')
                    
                    if len(odds_in_grandparent) in [2, 3]:
                        processed_grandparents.add(id(grandparent))
                        
                        # Extraire le texte complet du grand-parent
                        match = self._parse_match_from_bet_group(grandparent, competition)
                        if match:
                            matches.append(match)
        
        return matches
    
    def _parse_match_from_bet_group(self, elem, competition: str) -> Optional[Match]:
        """Parse un groupe de paris pour extraire le match"""
        try:
            # R√©cup√©rer le texte avec s√©parateurs
            text = elem.get_text('|', strip=True)
            
            if not text or len(text) < 10:
                return None
            
            # Pattern Winamax: "XXX|√âquipe1|cote1|Match nul|cote2|√âquipe2|cote3"
            # ou parfois: "√âquipe1|cote1|N|cote2|√âquipe2|cote3"
            
            parts = [p.strip() for p in text.split('|') if p.strip()]
            
            # Trouver les cotes (format X,XX ou X.XX)
            odds_pattern = r'^(\d{1,2}[,\.]\d{1,2})$'
            
            odds_indices = []
            for i, part in enumerate(parts):
                if re.match(odds_pattern, part):
                    odds_indices.append(i)
            
            if len(odds_indices) < 2:
                return None
            
            # Les 2 ou 3 premi√®res cotes trouv√©es
            odds_values = []
            for idx in odds_indices[:3]:
                val = float(parts[idx].replace(',', '.'))
                if 1.01 <= val <= 100:
                    odds_values.append(val)
            
            if len(odds_values) not in [2, 3]:
                return None
            
            if len(odds_values) == 3:
                odds_home = odds_values[0]
                odds_draw = odds_values[1]
                odds_away = odds_values[2]
            else:
                odds_home = odds_values[0]
                odds_draw = 1.0  # Pas de nul
                odds_away = odds_values[1]
            
            # Trouver les √©quipes
            # Pattern Winamax 3 issues: "index|√©quipe1|cote1|Match nul|cote2|√©quipe2|cote3"
            # Pattern Winamax 2 issues: "index|√©quipe1|cote1|√©quipe2|cote2"
            
            first_odds_idx = odds_indices[0]
            # Pour 2 issues, la 2√®me cote est √† l'index 1
            last_odds_idx = odds_indices[2] if len(odds_indices) >= 3 else odds_indices[1]
            
            # √âquipe domicile: juste avant la 1√®re cote
            home_team = None
            for i in range(first_odds_idx - 1, -1, -1):
                candidate = parts[i]
                # Ignorer "Match nul", "N", les nombres seuls, et les pourcentages
                if candidate.lower() not in ['match nul', 'n', 'nul', '1', '2', 'x']:
                    if not re.match(r'^[\d,\.%]+$', candidate):  # Exclure aussi les %
                        if len(candidate) > 2:  # Un nom d'√©quipe a au moins 3 caract√®res
                            home_team = candidate
                            break
            
            # √âquipe ext√©rieur: ENTRE la 1√®re et derni√®re cote utilis√©e
            # Pour 3 issues: entre cote 2 et cote 3
            # Pour 2 issues: entre cote 1 et cote 2
            away_team = None
            
            if len(odds_values) == 3:
                start_search = odds_indices[1] + 1
                end_search = odds_indices[2]
            else:
                start_search = odds_indices[0] + 1
                end_search = odds_indices[1]

            for i in range(start_search, end_search):
                candidate = parts[i]
                if candidate.lower() not in ['match nul', 'n', 'nul', '1', '2', 'x']:
                    if not re.match(r'^[\d,\.%]+$', candidate):  # Exclure aussi les %
                        if len(candidate) > 2:  # Un nom d'√©quipe a au moins 3 caract√®res
                            away_team = candidate
                            break
            
            if not home_team or not away_team:
                return None
            
            # Nettoyer les noms d'√©quipes
            home_team = home_team[:40].strip()
            away_team = away_team[:40].strip()
            
            if home_team == away_team or len(home_team) < 2 or len(away_team) < 2:
                return None
            
            # Normalisation pour l'ID unique (gestion A vs B / B vs A et clean noms)
            def clean_name(name):
                # Enlever les virgules invers√©es "Nom, Pr√©nom" -> "Pr√©nom Nom"
                if ',' in name:
                    parts = name.split(',')
                    if len(parts) == 2:
                        return f"{parts[1].strip()} {parts[0].strip()}".lower()
                return name.lower().replace(',', '').strip()
            
            h_clean = clean_name(home_team)
            a_clean = clean_name(away_team)
            
            # ID ind√©pendant de l'ordre domicile/ext√©rieur
            teams_sorted = sorted([h_clean, a_clean])
            match_id = f"winamax_{teams_sorted[0][:10]}_{teams_sorted[1][:10]}"
            
            # D√©tection et exclusion explicite des paris "Set" ou "Jeu" ou "Point" dans le texte
            # (Heuristique simple: si le texte original contient ces mots, c'est probablement pas le vainqueur du match du tout d√©but)
            full_text = elem.get_text().lower()
            if "set " in full_text or "jeu " in full_text or "point " in full_text or "exact" in full_text:
                # On risque de filtrer trop, mais c'est plus s√ªr pour √©viter les doublons de paris annexes
                # Pour le moment, on se fie au d√©doublonnage par ID (on garde le premier trouv√©)
                pass 

            return Match(
                id=match_id,
                competition=competition,
                home_team=home_team,
                away_team=away_team,
                date="",
                odds_home=odds_home,
                odds_draw=odds_draw,
                odds_away=odds_away,
                bookmaker=self.BOOKMAKER_NAME,
                url=self.driver.current_url if self.driver else ""
            )
            
        except Exception as e:
            return None


# ============================================================================
# Fonctions utilitaires export√©es
# ============================================================================

def get_best_matches(limit: int = 30) -> List[Match]:
    """R√©cup√®re et classe les meilleurs matchs Winamax"""
    scraper = WinamaxScraper(headless=True)
    result = scraper.scrape()
    
    if not result.matches:
        print("\n‚ö†Ô∏è Impossible de scraper Winamax.")
        return []
    
    matches = sorted(result.matches, key=lambda m: m.min_odds, reverse=True)
    return matches[:limit]


def get_matches_as_json() -> dict:
    """Retourne les matchs Winamax en JSON pour l'API"""
    scraper = WinamaxScraper(headless=True)
    result = scraper.scrape()
    
    return {
        "bookmaker": "Winamax",
        "status": result.status,
        "message": result.message,
        "count": result.count,
        "timestamp": result.timestamp,
        "duration_seconds": result.duration_seconds,
        "matches": [m.to_dict() for m in sorted(result.matches, key=lambda m: m.min_odds, reverse=True)]
    }


if __name__ == "__main__":
    matches = get_best_matches(20)
    display_matches(matches, limit=20)
